{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting weather forecasting model...\n",
      "Loading training data...\n",
      "Loading test data...\n",
      "Train data shape: (84960, 10)\n",
      "Test data shape: (4530, 5)\n"
     ]
    }
   ],
   "source": [
    "# Function to safely read CSV files with error handling\n",
    "def safe_read_csv(file_path):\n",
    "    try:\n",
    "        # Try reading with default settings\n",
    "        df = pd.read_csv(file_path)\n",
    "        return df, None\n",
    "    except Exception as e1:\n",
    "        try:\n",
    "            # Try reading with explicit encoding\n",
    "            df = pd.read_csv(file_path, encoding='utf-8')\n",
    "            return df, None\n",
    "        except Exception as e2:\n",
    "            try:\n",
    "                # Try reading with different separator\n",
    "                df = pd.read_csv(file_path, sep='\\t')\n",
    "                return df, None\n",
    "            except Exception as e3:\n",
    "                return None, f\"Failed to read {file_path}: {str(e3)}\"\n",
    "\n",
    "# Display progress information\n",
    "print(\"Starting weather forecasting model...\")\n",
    "\n",
    "# Load datasets with error handling\n",
    "print(\"Loading training data...\")\n",
    "train, train_error = safe_read_csv('train.csv')\n",
    "if train_error:\n",
    "    print(train_error)\n",
    "    exit(1)\n",
    "\n",
    "print(\"Loading test data...\")\n",
    "test, test_error = safe_read_csv('test.csv')\n",
    "if test_error:\n",
    "    print(test_error)\n",
    "    exit(1)\n",
    "\n",
    "print(f\"Train data shape: {train.shape}\")\n",
    "print(f\"Test data shape: {test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the required columns exist\n",
    "required_train_cols = ['ID', 'Year', 'Month', 'Day', 'kingdom', 'Avg_Temperature', \n",
    "                       'Radiation', 'Rain_Amount', 'Wind_Speed', 'Wind_Direction']\n",
    "required_test_cols = ['ID', 'Year', 'Month', 'Day', 'kingdom']\n",
    "\n",
    "# Check train columns\n",
    "missing_train_cols = [col for col in required_train_cols if col not in train.columns]\n",
    "if missing_train_cols:\n",
    "    print(f\"Error: Training data missing required columns: {missing_train_cols}\")\n",
    "    print(f\"Available columns: {train.columns.tolist()}\")\n",
    "    exit(1)\n",
    "\n",
    "# Check test columns\n",
    "missing_test_cols = [col for col in required_test_cols if col not in test.columns]\n",
    "if missing_test_cols:\n",
    "    print(f\"Error: Test data missing required columns: {missing_test_cols}\")\n",
    "    print(f\"Available columns: {test.columns.tolist()}\")\n",
    "    exit(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data...\n",
      "Converting temperature units...\n",
      "Creating features...\n",
      "Calculating kingdom statistics...\n",
      "Merging features...\n",
      "Defining feature sets...\n"
     ]
    }
   ],
   "source": [
    "# Clean and prepare data\n",
    "print(\"Preparing data...\")\n",
    "\n",
    "# Handling duplicate rows if any\n",
    "train = train.drop_duplicates()\n",
    "test = test.drop_duplicates()\n",
    "\n",
    "# Check for and remove rows with all missing values\n",
    "train = train.dropna(how='all')\n",
    "test = test.dropna(how='all')\n",
    "\n",
    "# Ensure numeric values are properly formatted\n",
    "for col in ['Year', 'Month', 'Day']:\n",
    "    train[col] = pd.to_numeric(train[col], errors='coerce')\n",
    "    test[col] = pd.to_numeric(test[col], errors='coerce')\n",
    "\n",
    "# Drop rows with invalid date components\n",
    "train = train.dropna(subset=['Year', 'Month', 'Day'])\n",
    "test = test.dropna(subset=['Year', 'Month', 'Day'])\n",
    "\n",
    "# Convert to appropriate types\n",
    "train['Year'] = train['Year'].astype(int)\n",
    "train['Month'] = train['Month'].astype(int)\n",
    "train['Day'] = train['Day'].astype(int)\n",
    "test['Year'] = test['Year'].astype(int)\n",
    "test['Month'] = test['Month'].astype(int)\n",
    "test['Day'] = test['Day'].astype(int)\n",
    "\n",
    "# Handle the temperature unit issue - convert Kelvin to Celsius\n",
    "print(\"Converting temperature units...\")\n",
    "temp_mask = train['Avg_Temperature'] > 100  # Threshold to identify Kelvin values\n",
    "train.loc[temp_mask, 'Avg_Temperature'] = train.loc[temp_mask, 'Avg_Temperature'] - 273.15\n",
    "\n",
    "# Create date keys for sorting (avoid datetime conversion issues)\n",
    "train['DateKey'] = train['Year']*10000 + train['Month']*100 + train['Day']\n",
    "test['DateKey'] = test['Year']*10000 + test['Month']*100 + test['Day']\n",
    "\n",
    "# Generate temporal features\n",
    "print(\"Creating features...\")\n",
    "for df in [train, test]:\n",
    "    # Create approximation for day of year\n",
    "    df['DayOfYear'] = (df['Month'] - 1) * 30 + df['Day']\n",
    "    df['MonthDay'] = df['Day']\n",
    "    df['Season'] = (df['Month'] % 12 + 3) // 3  # 1: Spring, 2: Summer, 3: Fall, 4: Winter\n",
    "    # Create month-day combination for seasonal patterns\n",
    "    df['MonthDay_Combined'] = df['Month'] * 100 + df['Day']\n",
    "\n",
    "# Encode categorical variables\n",
    "try:\n",
    "    le = LabelEncoder()\n",
    "    # Handle potential encoding errors by forcing string type\n",
    "    train['kingdom_str'] = train['kingdom'].astype(str)\n",
    "    test['kingdom_str'] = test['kingdom'].astype(str)\n",
    "    train['kingdom_encoded'] = le.fit_transform(train['kingdom_str'])\n",
    "    test['kingdom_encoded'] = le.transform(test['kingdom_str'])\n",
    "except Exception as e:\n",
    "    print(f\"Error encoding kingdom: {str(e)}\")\n",
    "    # Fallback: use numeric encoding if LabelEncoder fails\n",
    "    kingdom_mapping = {k: i for i, k in enumerate(train['kingdom'].unique())}\n",
    "    train['kingdom_encoded'] = train['kingdom'].map(kingdom_mapping)\n",
    "    test['kingdom_encoded'] = test['kingdom'].map(kingdom_mapping)\n",
    "    # Fill missing values with -1\n",
    "    test['kingdom_encoded'] = test['kingdom_encoded'].fillna(-1).astype(int)\n",
    "\n",
    "# Target columns\n",
    "target_cols = ['Avg_Temperature', 'Radiation', 'Rain_Amount', 'Wind_Speed', 'Wind_Direction']\n",
    "\n",
    "# Create a submission DataFrame\n",
    "model_predictions = pd.DataFrame({'ID': test['ID']})\n",
    "\n",
    "# Calculate kingdom-specific statistics\n",
    "print(\"Calculating kingdom statistics...\")\n",
    "kingdom_data = train.groupby('kingdom').agg({\n",
    "    'Avg_Temperature': ['mean', 'median', 'std'],\n",
    "    'Radiation': ['mean', 'median', 'std'],\n",
    "    'Rain_Amount': ['mean', 'median', 'std'],\n",
    "    'Wind_Speed': ['mean', 'median', 'std'],\n",
    "    'Wind_Direction': ['mean', 'median', 'std']\n",
    "})\n",
    "\n",
    "# Flatten the multi-index columns\n",
    "kingdom_data.columns = ['_'.join(col).strip() for col in kingdom_data.columns.values]\n",
    "kingdom_data = kingdom_data.reset_index()\n",
    "\n",
    "# Calculate month-day specific statistics (seasonal patterns)\n",
    "monthday_data = train.groupby(['Month', 'Day']).agg({\n",
    "    'Avg_Temperature': 'mean',\n",
    "    'Radiation': 'mean',\n",
    "    'Rain_Amount': 'mean', \n",
    "    'Wind_Speed': 'mean',\n",
    "    'Wind_Direction': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "# Merge kingdom statistics to test data\n",
    "print(\"Merging features...\")\n",
    "test = pd.merge(test, kingdom_data, on='kingdom', how='left')\n",
    "\n",
    "# Merge month-day statistics to test data\n",
    "test = pd.merge(test, monthday_data, on=['Month', 'Day'], how='left', \n",
    "                suffixes=('', '_monthday_mean'))\n",
    "\n",
    "# Fill any missing values from the merges\n",
    "for col in test.columns:\n",
    "    if test[col].isna().any():\n",
    "        if col.endswith('_mean') or col.endswith('_median'):\n",
    "            # For statistic columns, fill with the average of that statistic\n",
    "            test[col] = test[col].fillna(test[col].mean())\n",
    "        elif col in target_cols:\n",
    "            # For direct target columns, use global mean\n",
    "            test[col] = test[col].fillna(train[col].mean())\n",
    "\n",
    "# Define features based on what's available\n",
    "print(\"Defining feature sets...\")\n",
    "base_features = ['Year', 'Month', 'Day', 'kingdom_encoded', 'DayOfYear', 'Season']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing data...\n",
      "Processing Avg_Temperature...\n",
      "Processing Radiation...\n",
      "Processing Rain_Amount...\n",
      "Processing Wind_Speed...\n",
      "Processing Wind_Direction...\n",
      "Preprocessing complete.\n"
     ]
    }
   ],
   "source": [
    "print(\"Preprocessing data...\")\n",
    "preprocessed_data = {}\n",
    "\n",
    "for target in target_cols:\n",
    "    print(f\"Processing {target}...\")\n",
    "    features = base_features.copy()\n",
    "    \n",
    "    for stat in ['mean', 'median', 'std']:\n",
    "        col_name = f\"{target}_{stat}\"\n",
    "        if col_name in test.columns:\n",
    "            features.append(col_name)\n",
    "    \n",
    "    col_name = f\"{target}_monthday_mean\"\n",
    "    if col_name in test.columns:\n",
    "        features.append(col_name)\n",
    "    \n",
    "    features = [f for f in features if f in train.columns and f in test.columns]\n",
    "    \n",
    "    X_train = train[features].fillna(0)\n",
    "    y_train = train[target].fillna(train[target].mean())\n",
    "    X_test = test[features].fillna(0)\n",
    "    \n",
    "    if y_train.isna().any():\n",
    "        print(f\"  Warning: Found {y_train.isna().sum()} NaN values in {target}. Filling with mean.\")\n",
    "        y_train = y_train.fillna(y_train.mean())\n",
    "    \n",
    "    preprocessed_data[target] = {'X_train': X_train, 'y_train': y_train, 'X_test': X_test}\n",
    "\n",
    "print(\"Preprocessing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train and predict with any model\n",
    "def train_and_predict(model):\n",
    "    print(f\"Training {model.__class__.__name__} model and making predictions...\")\n",
    "    \n",
    "    predictions_df = pd.DataFrame({'ID': test['ID']})\n",
    "    for target, data in preprocessed_data.items():\n",
    "        print(f\"  Training {model.__class__.__name__} for {target}...\")\n",
    "        \n",
    "        try:\n",
    "            model.fit(data['X_train'], data['y_train'])\n",
    "            predictions = model.predict(data['X_test'])\n",
    "        except Exception as e:\n",
    "            print(f\"  Error training {model.__class__.__name__} for {target}: {str(e)}\")\n",
    "            predictions = test[f\"{target}_mean\"].values\n",
    "        \n",
    "        predictions_df[f\"{target}\"] = predictions\n",
    "    \n",
    "    print(\"Post-processing predictions...\")\n",
    "    for target in ['Radiation', 'Rain_Amount', 'Wind_Speed']:\n",
    "        predictions_df[f\"{target}\"] = predictions_df[f\"{target}\"].clip(lower=0)\n",
    "\n",
    "    predictions_df['Wind_Direction'] %= 360\n",
    "    \n",
    "    return predictions_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training RandomForestRegressor model and making predictions...\n",
      "  Training RandomForestRegressor for Avg_Temperature...\n",
      "  Training RandomForestRegressor for Radiation...\n",
      "  Training RandomForestRegressor for Rain_Amount...\n",
      "  Training RandomForestRegressor for Wind_Speed...\n",
      "  Training RandomForestRegressor for Wind_Direction...\n",
      "Post-processing predictions...\n",
      "Saving submission file...\n",
      "Submission file created successfully!\n"
     ]
    }
   ],
   "source": [
    "# RandomForestRegressor\n",
    "random_forest_model = RandomForestRegressor(n_estimators=100, max_depth=12, min_samples_split=5, min_samples_leaf=2, random_state=42, n_jobs=-1)\n",
    "random_forest_predictions = train_and_predict(random_forest_model)\n",
    "\n",
    "print(\"Saving submission file...\")\n",
    "random_forest_predictions.to_csv('submission 1.csv', index=False)\n",
    "print(\"Submission file created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LGBMRegressor model and making predictions...\n",
      "  Training LGBMRegressor for Avg_Temperature...\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001195 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 344\n",
      "[LightGBM] [Info] Number of data points in the train set: 84960, number of used features: 6\n",
      "[LightGBM] [Info] Start training from score 26.340751\n",
      "  Training LGBMRegressor for Radiation...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002880 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 344\n",
      "[LightGBM] [Info] Number of data points in the train set: 84960, number of used features: 6\n",
      "[LightGBM] [Info] Start training from score 20.338598\n",
      "  Training LGBMRegressor for Rain_Amount...\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001862 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 344\n",
      "[LightGBM] [Info] Number of data points in the train set: 84960, number of used features: 6\n",
      "[LightGBM] [Info] Start training from score 7.723850\n",
      "  Training LGBMRegressor for Wind_Speed...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002696 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 344\n",
      "[LightGBM] [Info] Number of data points in the train set: 84960, number of used features: 6\n",
      "[LightGBM] [Info] Start training from score 15.629291\n",
      "  Training LGBMRegressor for Wind_Direction...\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000825 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 344\n",
      "[LightGBM] [Info] Number of data points in the train set: 84960, number of used features: 6\n",
      "[LightGBM] [Info] Start training from score 215.831297\n",
      "Post-processing predictions...\n",
      "Saving submission file...\n",
      "Submission file created successfully!\n"
     ]
    }
   ],
   "source": [
    "# LightGBM\n",
    "lgbm_model = lgb.LGBMRegressor(\n",
    "            n_estimators=200,\n",
    "            learning_rate=0.05,\n",
    "            num_leaves=31,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            min_child_samples=20,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    ")\n",
    "lgbm_predictions = train_and_predict(lgbm_model)\n",
    "\n",
    "print(\"Saving submission file...\")\n",
    "lgbm_predictions.to_csv('submission 2.csv', index=False)\n",
    "print(\"Submission file created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training XGBRegressor model and making predictions...\n",
      "  Training XGBRegressor for Avg_Temperature...\n",
      "  Training XGBRegressor for Radiation...\n",
      "  Training XGBRegressor for Rain_Amount...\n",
      "  Training XGBRegressor for Wind_Speed...\n",
      "  Training XGBRegressor for Wind_Direction...\n",
      "Post-processing predictions...\n",
      "Saving submission file...\n",
      "Submission file created successfully!\n"
     ]
    }
   ],
   "source": [
    "# XGBoost\n",
    "xgb_model = xgb.XGBRegressor(\n",
    "        n_estimators=200,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=8,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        min_child_weight=3,\n",
    "        objective='reg:squarederror',\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    ")\n",
    "xgb_predictions = train_and_predict(xgb_model)\n",
    "print(\"Saving submission file...\")\n",
    "xgb_predictions.to_csv('submission 3.csv', index=False)\n",
    "print(\"Submission file created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training GradientBoostingRegressor model and making predictions...\n",
      "  Training GradientBoostingRegressor for Avg_Temperature...\n",
      "  Training GradientBoostingRegressor for Radiation...\n",
      "  Training GradientBoostingRegressor for Rain_Amount...\n",
      "  Training GradientBoostingRegressor for Wind_Speed...\n",
      "  Training GradientBoostingRegressor for Wind_Direction...\n",
      "Post-processing predictions...\n",
      "Saving submission file...\n",
      "Submission file created successfully!\n"
     ]
    }
   ],
   "source": [
    "# GradientBoostingRegressor\n",
    "gradient_boost_model = GradientBoostingRegressor(\n",
    "        n_estimators=200,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=5,\n",
    "        subsample=0.8,\n",
    "        min_samples_leaf=20,\n",
    "        random_state=42\n",
    ")\n",
    "gradient_boost_predictions = train_and_predict(gradient_boost_model)\n",
    "print(\"Saving submission file...\")\n",
    "gradient_boost_predictions.to_csv('submission 5.csv', index=False)\n",
    "print(\"Submission file created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training BaggingRegressor model and making predictions...\n",
      "  Training BaggingRegressor for Avg_Temperature...\n",
      "  Training BaggingRegressor for Radiation...\n",
      "  Training BaggingRegressor for Rain_Amount...\n",
      "  Training BaggingRegressor for Wind_Speed...\n",
      "  Training BaggingRegressor for Wind_Direction...\n",
      "Post-processing predictions...\n",
      "Saving submission file...\n",
      "Submission file created successfully!\n"
     ]
    }
   ],
   "source": [
    "# BaggingRegressor\n",
    "bagging_model = BaggingRegressor(  # Changed from BaggingClassifier\n",
    "        estimator=DecisionTreeRegressor(),  # Changed from DecisionTreeClassifier\n",
    "        n_estimators=200,  \n",
    "        max_samples=0.8,  \n",
    "        max_features=0.8,  \n",
    "        random_state=42,  \n",
    "        n_jobs=-1  \n",
    ")\n",
    "bagging_predictions = train_and_predict(bagging_model)\n",
    "print(\"Saving submission file...\")\n",
    "bagging_predictions.to_csv('submission 6.csv', index=False)\n",
    "print(\"Submission file created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sMAPE for Avg_Temperature: 3.6305800561746935\n",
      "sMAPE for Radiation: 9.536708837289076\n",
      "sMAPE for Rain_Amount: 70.64783777298747\n",
      "sMAPE for Wind_Speed: 18.365474823623227\n",
      "sMAPE for Wind_Direction: 19.736307893407645\n",
      "Mean sMAPE across all targets: 24.383381876696426\n"
     ]
    }
   ],
   "source": [
    "def smape(y_true, y_pred):\n",
    "    denominator = np.abs(y_true) + np.abs(y_pred)\n",
    "    diff = np.abs(y_true - y_pred) / denominator\n",
    "    diff[denominator == 0] = 0.0  # Handle division by zero\n",
    "    return 200 * np.mean(diff)\n",
    "\n",
    "rf_smapes = []\n",
    "\n",
    "for target in target_cols:\n",
    "    actual_values = test[target]  # Replace with the actual column for each target\n",
    "    predicted_values = random_forest_predictions[target]  # Predictions for the target column\n",
    "    \n",
    "    # Calculate sMAPE for the current target\n",
    "    smape_score = smape(actual_values, predicted_values)\n",
    "    rf_smapes.append(smape_score)\n",
    "    print(f\"sMAPE for {target}: {smape_score}\")\n",
    "\n",
    "# Calculate mean sMAPE for all targets\n",
    "mean_rf_smape = np.mean(rf_smapes)\n",
    "print(f\"Mean sMAPE across all targets: {mean_rf_smape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sMAPE for Avg_Temperature: 3.8036742338691982\n",
      "sMAPE for Radiation: 5.376964820849653\n",
      "sMAPE for Rain_Amount: 42.1519458841763\n",
      "sMAPE for Wind_Speed: 16.280610662168634\n",
      "sMAPE for Wind_Direction: 14.090709629811476\n",
      "Mean sMAPE across all targets: 16.34078104617505\n"
     ]
    }
   ],
   "source": [
    "lgbm_smapes = []\n",
    "\n",
    "for target in target_cols:\n",
    "    actual_values = test[target]  # Replace with the actual column for each target\n",
    "    predicted_values = lgbm_predictions[target]  # Predictions for the target column\n",
    "    \n",
    "    # Calculate sMAPE for the current target\n",
    "    smape_score = smape(actual_values, predicted_values)\n",
    "    lgbm_smapes.append(smape_score)\n",
    "    print(f\"sMAPE for {target}: {smape_score}\")\n",
    "\n",
    "# Calculate mean sMAPE for all targets\n",
    "mean_lgbm_smape = np.mean(lgbm_smapes)\n",
    "print(f\"Mean sMAPE across all targets: {mean_lgbm_smape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sMAPE for XGBoost Avg_Temperature: 4.1144389268921815\n",
      "sMAPE for XGBoost Radiation: 8.598202602659095\n",
      "sMAPE for XGBoost Rain_Amount: 67.18132755632526\n",
      "sMAPE for XGBoost Wind_Speed: 19.218896985454315\n",
      "sMAPE for XGBoost Wind_Direction: 18.660414124513764\n",
      "Mean sMAPE across all targets for XGBoost: 23.554656039168922\n"
     ]
    }
   ],
   "source": [
    "# For XGBoost model\n",
    "xgb_smapes = []\n",
    "\n",
    "for target in target_cols:\n",
    "    actual_values = test[target]  # Actual values from the test set for each target\n",
    "    predicted_values = xgb_predictions[target]  # Predictions from the xgb model\n",
    "    \n",
    "    # Calculate sMAPE for the current target\n",
    "    smape_score = smape(actual_values, predicted_values)\n",
    "    xgb_smapes.append(smape_score)\n",
    "    print(f\"sMAPE for XGBoost {target}: {smape_score}\")\n",
    "\n",
    "# Calculate mean sMAPE for XGBoost\n",
    "mean_xgb_smape = np.mean(xgb_smapes)\n",
    "print(f\"Mean sMAPE across all targets for XGBoost: {mean_xgb_smape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sMAPE for Gradient Boosting Avg_Temperature: 3.680698875864695\n",
      "sMAPE for Gradient Boosting Radiation: 5.93320036555565\n",
      "sMAPE for Gradient Boosting Rain_Amount: 46.595133569817676\n",
      "sMAPE for Gradient Boosting Wind_Speed: 17.14750097301889\n",
      "sMAPE for Gradient Boosting Wind_Direction: 13.97850797705458\n",
      "Mean sMAPE across all targets for Gradient Boosting: 17.467008352262297\n"
     ]
    }
   ],
   "source": [
    "# For Gradient Boosting model\n",
    "gb_smapes = []\n",
    "\n",
    "for target in target_cols:\n",
    "    actual_values = test[target]  # Actual values from the test set for each target\n",
    "    predicted_values = gradient_boost_predictions[target]  # Predictions from the gradient boosting model\n",
    "    \n",
    "    # Calculate sMAPE for the current target\n",
    "    smape_score = smape(actual_values, predicted_values)\n",
    "    gb_smapes.append(smape_score)\n",
    "    print(f\"sMAPE for Gradient Boosting {target}: {smape_score}\")\n",
    "\n",
    "# Calculate mean sMAPE for Gradient Boosting\n",
    "mean_gb_smape = np.mean(gb_smapes)\n",
    "print(f\"Mean sMAPE across all targets for Gradient Boosting: {mean_gb_smape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sMAPE for Bagging Avg_Temperature: 2.8276818848162777\n",
      "sMAPE for Bagging Radiation: 5.979694847130214\n",
      "sMAPE for Bagging Rain_Amount: 43.18520899702037\n",
      "sMAPE for Bagging Wind_Speed: 14.296864634498768\n",
      "sMAPE for Bagging Wind_Direction: 12.6984132895259\n",
      "Mean sMAPE across all targets for Bagging: 15.797572730598304\n"
     ]
    }
   ],
   "source": [
    "# For Bagging model\n",
    "bagging_smapes = []\n",
    "\n",
    "for target in target_cols:\n",
    "    actual_values = test[target]  # Actual values from the test set for each target\n",
    "    predicted_values = bagging_predictions[target]  # Predictions from the bagging model\n",
    "    \n",
    "    # Calculate sMAPE for the current target\n",
    "    smape_score = smape(actual_values, predicted_values)\n",
    "    bagging_smapes.append(smape_score)\n",
    "    print(f\"sMAPE for Bagging {target}: {smape_score}\")\n",
    "\n",
    "# Calculate mean sMAPE for Bagging\n",
    "mean_bagging_smape = np.mean(bagging_smapes)\n",
    "print(f\"Mean sMAPE across all targets for Bagging: {mean_bagging_smape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
